{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from bandits.environment.cascade.context_free import CascadeContextFreeBandit\n",
    "from bandits.policy.context_free import BernoulliTS, Random\n",
    "from bandits.plotting import plot_beta_dist\n",
    "import vegafusion as vf\n",
    "from typing import TypedDict, Union\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.enable('json') # saves the data locally in .json file so notebook doesn't get large\n",
    "# or can allow altair to keep the data in the notebook using alt.data_transformers.disable_max_rows() but it will create large notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf_vs_actual(\n",
    "    pdf_df: pd.DataFrame,\n",
    "    actuals_df: pd.DataFrame,\n",
    "    width: int =500,\n",
    "    height: int =250\n",
    ") -> alt.Chart:\n",
    "        \n",
    "    pdf_charts = alt.Chart(pdf_df).mark_line().encode(\n",
    "        y=alt.Y('pdf'),\n",
    "        x=alt.X('x', title='θ'),\n",
    "        color=alt.Color('action:N', legend=None),\n",
    "        tooltip = [\n",
    "            'action',\n",
    "            alt.Tooltip('pdf', format='0.4', title='θ'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    actual_charts = alt.Chart(actuals_df).mark_rule().encode(\n",
    "        x=alt.X('w', title='θ'),\n",
    "        color=alt.Color('action:N', legend=None),\n",
    "        tooltip = [\n",
    "            'action',\n",
    "            alt.Tooltip('w', format='0.4', title='θ'),\n",
    "        ]        \n",
    "    )\n",
    "\n",
    "    final_chart = (pdf_charts + actual_charts).properties(\n",
    "        width=width, height=height\n",
    "    )\n",
    "\n",
    "    return final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(\n",
    "    policy: BernoulliTS,\n",
    "    env: CascadeContextFreeBandit\n",
    ") -> alt.Chart:\n",
    "\n",
    "    pred = (policy.reward_counts / policy.action_counts)\n",
    "    act = env.weights\n",
    "\n",
    "    chart_df = pd.DataFrame(dict(pred=pred, act=act)).assign(\n",
    "        diff=lambda x: (x.act - x.pred).abs()\n",
    "    ).assign(\n",
    "        arm=lambda x: x.index,\n",
    "        optimal_arm=lambda x: x.arm.isin(env.optimal_action),\n",
    "        act_sort=lambda x: x.act,\n",
    "    ).sort_values(\n",
    "        ['diff'], ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    chart_ff_df = chart_df.melt(id_vars=['arm', 'optimal_arm', 'diff', 'act_sort'])\n",
    "\n",
    "    return alt.Chart(chart_ff_df).mark_point().encode(\n",
    "        y=alt.Y('arm:O', sort=alt.SortField(\"act_sort\", \"descending\")),\n",
    "        x=alt.X('value'),\n",
    "        color=alt.Color('variable'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observed_optimal_action_prob(\n",
    "    reporting_df: pd.DataFrame,\n",
    "    height: int = 275,\n",
    "    width: int = 675,    \n",
    ") -> tuple[pd.DataFrame, alt.Chart]:\n",
    "    policy_prob_df = reporting_df.assign(\n",
    "        n_trials=1\n",
    "    ).groupby(\n",
    "        ['policy_batch_check'], as_index=False\n",
    "    )[['n_trials','optimal_action_id']].sum().assign(\n",
    "        prob_of_optimal_action=lambda x: x['optimal_action_id'] /  x['n_trials']\n",
    "    )\n",
    "\n",
    "    chart = alt.Chart(policy_prob_df).mark_line().encode(\n",
    "        x='policy_batch_check', y='prob_of_optimal_action'\n",
    "    ).properties(\n",
    "        width=width, \n",
    "        height=height,\n",
    "    )\n",
    "    return policy_prob_df, chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf_with_actuals(\n",
    "    policy: BernoulliTS,\n",
    "    env: CascadeContextFreeBandit,\n",
    "    width: int = 500,\n",
    "    height: int = 250,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, alt.Chart]:\n",
    "    actuals_df = pd.DataFrame(dict(w=env.weights)).assign(action=lambda x: x.index)\n",
    "\n",
    "    all_pdf = []\n",
    "    for idx in range(policy.n_actions):\n",
    "        pdf = plot_beta_dist(\n",
    "            alpha=policy.alpha[idx] + policy.reward_counts[idx],\n",
    "            beta=policy.beta[idx] + (policy.action_counts[idx] - policy.reward_counts[idx])\n",
    "        ).assign(\n",
    "            action=idx,\n",
    "        )\n",
    "        all_pdf.append(pdf)\n",
    "\n",
    "    all_pdf_df = pd.concat(all_pdf, axis=0).reset_index(drop=True)    \n",
    "\n",
    "    pdf_charts = alt.Chart(all_pdf_df).mark_line().encode(\n",
    "        y=alt.Y('pdf'),\n",
    "        x=alt.X('x', title='θ'),\n",
    "        color=alt.Color('action:N', legend=None)\n",
    "    )\n",
    "\n",
    "    actual_charts = alt.Chart(actuals_df).mark_rule().encode(\n",
    "        x=alt.X('w', title='θ'),\n",
    "        color=alt.Color('action:N', legend=None)\n",
    "    )\n",
    "\n",
    "    final_chart = (pdf_charts + actual_charts).properties(\n",
    "        width=width, height=height\n",
    "    )\n",
    "\n",
    "    return all_pdf_df, actuals_df, final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionRewardLogging(TypedDict):\n",
    "    action: list[int]\n",
    "    reward: float\n",
    "    prob_of_click: float\n",
    "\n",
    "def harmonise_reporting(\n",
    "    reporting: list[ActionRewardLogging],\n",
    "    env: CascadeContextFreeBandit,\n",
    "    policy_batch_check: int = 500,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    reporting_df = pd.DataFrame(reporting).assign(\n",
    "        optimal_prob_of_click=env.optimal_reward,\n",
    "        time_idx=lambda x: x.index\n",
    "    )\n",
    "    reporting_df['cumulative_reward'] = reporting_df['reward'].cumsum()\n",
    "    reporting_df['avg_cumulative_reward'] = reporting_df['cumulative_reward'] / (reporting_df['time_idx'] + 1)\n",
    "    reporting_df['avg_cumulative_reward'] = reporting_df['avg_cumulative_reward'].fillna(0)\n",
    "\n",
    "    reporting_df['action_as_str'] = reporting_df['action'].apply(lambda x: '|'.join([str(y) for y in x]))\n",
    "\n",
    "    reporting_df['optimal_action_id'] = reporting_df['action'].apply(lambda x: all(x == env.optimal_action))\n",
    "    reporting_df['cumulative_optimal_action_taken'] = reporting_df['optimal_action_id'].cumsum()\n",
    "    reporting_df['avg_cumulative_optimal_action_taken'] = reporting_df['cumulative_optimal_action_taken']/ (reporting_df['time_idx'] + 1)\n",
    "    reporting_df['avg_cumulative_optimal_action_taken'] = reporting_df['avg_cumulative_optimal_action_taken'].fillna(0)\n",
    "    \n",
    "    reporting_df['policy_batch_check'] = (reporting_df['time_idx']  - reporting_df['time_idx'] % policy_batch_check)\n",
    "    return reporting_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 50\n",
    "LEN_LIST = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CascadeContextFreeBandit(\n",
    "    weights=np.random.beta(a=1, b=99, size=N_ACTIONS),\n",
    "    max_steps=1_000_000,\n",
    "    len_list=LEN_LIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_of_policies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_df = pd.DataFrame(dict(w=env.weights)).assign(action=lambda x: x.index)\n",
    "actuals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = N_ACTIONS * 13\n",
    "actual_charts = alt.Chart(actuals_df).mark_rule().encode(\n",
    "    x=alt.X(\n",
    "        'action:N',\n",
    "        sort=alt.SortField(\"w\", order='descending'),\n",
    "        axis=alt.Axis(orient='bottom', labelAngle=0)\n",
    "    ),\n",
    "    color=alt.Color('action:N', legend=None),\n",
    "    y=alt.Y('w', title='θ'),\n",
    "    text=alt.Text('w', format='0.3'),\n",
    "    tooltip=[\n",
    "        'action',\n",
    "        alt.Tooltip('w', format='0.3', title='θ')\n",
    "    ]\n",
    ")\n",
    "\n",
    "final_chart = (\n",
    "    actual_charts +\n",
    "    actual_charts.mark_point(filled=True, size=50) + \n",
    "    actual_charts.mark_text(align='left', angle=45*7, dx=5)\n",
    ").properties(\n",
    "    width=width, height=225\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Random(\n",
    "    n_actions=env.n_actions,\n",
    "    len_list=env.len_list,\n",
    "    random_state=1234,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=1234)\n",
    "action = policy.select_action()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting = []\n",
    "\n",
    "while True:\n",
    "    _, reward, terminated, truncated, info = env.step(action=action)\n",
    "    reporting.append(dict(\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        prob_of_click=info[\"prob_of_click\"],        \n",
    "    ))\n",
    "\n",
    "    policy.cascade_params_update(\n",
    "        action=action,\n",
    "        reward_position=info[\"position_of_click\"]\n",
    "    )\n",
    "\n",
    "    if truncated:\n",
    "        break    \n",
    "\n",
    "    action = policy.select_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_df = harmonise_reporting(\n",
    "    reporting=reporting,\n",
    "    env=env,\n",
    "    policy_batch_check=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'random'\n",
    "outputs_of_policies[policy_name] = reporting_df.assign(policy=policy_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_ff_df = reporting_df.melt(\n",
    "    id_vars=['time_idx', 'action'],\n",
    "    value_vars=['prob_of_click', 'optimal_prob_of_click']\n",
    ")\n",
    "\n",
    "reporting_ff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 1_000\n",
    "mask_df = (reporting_df['time_idx'] % plot_every) == 0\n",
    "mask_ff_df = (reporting_ff_df['time_idx'] % plot_every) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_ff_df[mask_ff_df]).mark_line().encode(\n",
    "    y=alt.Y('value'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color='variable'\n",
    ").properties(width=600)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__prob_of_click.png\",\n",
    ")\n",
    "\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_df[mask_df]).mark_line().encode(\n",
    "    x='time_idx',\n",
    "    y='avg_cumulative_reward',\n",
    ").properties(\n",
    "    width=700, \n",
    ")\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__avg_reward.png\",\n",
    ")\n",
    "\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = plot_actual_vs_predicted(policy=policy, env=env)\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_probs_actual_vs_predicted.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_prob_df, chart = plot_observed_optimal_action_prob(\n",
    "    reporting_df=reporting_df,\n",
    "    height=275,\n",
    "    width=675\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_prob_df['optimal_action_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uninformed Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BernoulliTS(\n",
    "    n_actions=env.n_actions,\n",
    "    len_list=env.len_list,\n",
    "    random_state=1234,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_df = plot_beta_dist(alpha=1, beta=1)\n",
    "actuals_df = pd.DataFrame(dict(w=env.weights)).assign(action=lambda x: x.index)\n",
    "\n",
    "actuals_chart = alt.Chart(actuals_df).mark_rule(opacity=0.5).encode(\n",
    "    x=alt.X('w', title='θ'),\n",
    ")\n",
    "priors_chart = alt.Chart(prior_df).mark_area(opacity=0.5).encode(\n",
    "    x=alt.X('x', title='θ'),\n",
    "    y=alt.Y('pdf')\n",
    ")\n",
    "\n",
    "final_chart = (\n",
    "    priors_chart + actuals_chart\n",
    ").properties(\n",
    "    width=500, height=200\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=34325)\n",
    "action = policy.select_action()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting = []\n",
    "\n",
    "while True:\n",
    "    _, reward, terminated, truncated, info = env.step(action=action)\n",
    "    reporting.append(dict(\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        prob_of_click=info[\"prob_of_click\"],        \n",
    "    ))\n",
    "\n",
    "    policy.cascade_params_update(\n",
    "        action=action,\n",
    "        reward_position=info[\"position_of_click\"]\n",
    "    )\n",
    "\n",
    "    if truncated:\n",
    "        break    \n",
    "    \n",
    "    action = policy.select_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_df = harmonise_reporting(reporting=reporting, env=env, policy_batch_check=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'ts-priors-Beta(1, 1)'\n",
    "outputs_of_policies[policy_name] = reporting_df.assign(policy=policy_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_ff_df = reporting_df.melt(\n",
    "    id_vars=['time_idx', 'action'],\n",
    "    value_vars=['prob_of_click', 'optimal_prob_of_click']\n",
    ")\n",
    "\n",
    "reporting_ff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 100\n",
    "mask_df = (reporting_df['time_idx'] % plot_every) == 0\n",
    "mask_ff_df = (reporting_ff_df['time_idx'] % plot_every) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_ff_df[mask_ff_df]).mark_line().encode(\n",
    "    y=alt.Y('value'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color='variable'\n",
    ").properties(width=600)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__prob_of_click.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_df[mask_df]).mark_line().encode(\n",
    "    x='time_idx',\n",
    "    y='avg_cumulative_reward',\n",
    ").properties(\n",
    "    width=700, \n",
    ")\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__avg_reward.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = plot_actual_vs_predicted(policy=policy, env=env)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_probs_actual_vs_predicted.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_prob_df, chart = plot_observed_optimal_action_prob(\n",
    "    reporting_df=reporting_df,\n",
    "    height=275,\n",
    "    width=675\n",
    ")\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__observed_optimal_action_distribution.png\",\n",
    ")\n",
    "\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf_df, actuals_df, chart = plot_pdf_with_actuals(env=env, policy=policy)\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_beta_distributions.png\",\n",
    ")\n",
    "\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pessimistic priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BernoulliTS(\n",
    "    n_actions=env.n_actions,\n",
    "    len_list=env.len_list,\n",
    "    random_state=1234,\n",
    "    batch_size=1,\n",
    "    alpha=np.ones(env.n_actions) * 1,\n",
    "    beta=np.ones(env.n_actions) * 99,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_df = plot_beta_dist(alpha=1, beta=99)\n",
    "actuals_df = pd.DataFrame(dict(w=env.weights)).assign(action=lambda x: x.index)\n",
    "\n",
    "actuals_chart = alt.Chart(actuals_df).mark_rule(opacity=0.5).encode(\n",
    "    x=alt.X('w', title='θ'),\n",
    ")\n",
    "priors_chart = alt.Chart(prior_df).mark_area(opacity=0.5).encode(\n",
    "    x=alt.X('x', title='θ'),\n",
    "    y=alt.Y('pdf')\n",
    ")\n",
    "\n",
    "final_chart = (\n",
    "    priors_chart + actuals_chart\n",
    ").properties(\n",
    "    width=500, height=200\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=1234)\n",
    "action = policy.select_action()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting = []\n",
    "\n",
    "while True:\n",
    "    _, reward, terminated, truncated, info = env.step(action=action)\n",
    "    reporting.append(dict(\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        prob_of_click=info[\"prob_of_click\"],        \n",
    "    ))\n",
    "\n",
    "    policy.cascade_params_update(\n",
    "        action=action,\n",
    "        reward_position=info[\"position_of_click\"]\n",
    "    )\n",
    "\n",
    "    if truncated:\n",
    "        break    \n",
    "    \n",
    "    action = policy.select_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_df = harmonise_reporting(reporting=reporting, env=env, policy_batch_check=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'ts-priors-Beta(1, 99)'\n",
    "outputs_of_policies[policy_name] = reporting_df.assign(policy=policy_name)\n",
    "\n",
    "reporting_ff_df = reporting_df.melt(\n",
    "    id_vars=['time_idx', 'action'],\n",
    "    value_vars=['prob_of_click', 'optimal_prob_of_click']\n",
    ")\n",
    "\n",
    "reporting_ff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 100\n",
    "mask_df = (reporting_df['time_idx'] % plot_every) == 0\n",
    "mask_ff_df = (reporting_ff_df['time_idx'] % plot_every) == 0\n",
    "\n",
    "\n",
    "chart = alt.Chart(reporting_ff_df[mask_ff_df]).mark_line().encode(\n",
    "    y=alt.Y('value'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color='variable'\n",
    ").properties(width=600)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__prob_of_click.png\",\n",
    ")\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_df[mask_df]).mark_line().encode(\n",
    "    x='time_idx',\n",
    "    y='avg_cumulative_reward',\n",
    ").properties(\n",
    "    width=700, \n",
    ")\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__avg_reward.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = plot_actual_vs_predicted(policy=policy, env=env)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_probs_actual_vs_predicted.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_prob_df, chart = plot_observed_optimal_action_prob(\n",
    "    reporting_df=reporting_df,\n",
    "    height=275,\n",
    "    width=675\n",
    ")\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__observed_optimal_action_distribution.png\",\n",
    ")\n",
    "\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf_df, actuals_df, chart = plot_pdf_with_actuals(env=env, policy=policy)\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_beta_distributions.png\",\n",
    ")\n",
    "\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running for N episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(mean: float, beta: float) -> float:\n",
    "    \"\"\"\n",
    "    mean of Beta distribution is given by: alpha/(alpha+beta)\n",
    "    \n",
    "    We have re-written the formula to give alpha\n",
    "    as a function of the mean and beta\n",
    "\n",
    "    Args:\n",
    "        target_p (float): probability of success\n",
    "        beta (float): \n",
    "\n",
    "    \"\"\"\n",
    "    return ((mean/(1-mean)) * beta) + ((1/(1-mean)) * (1-2 * mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_policy(\n",
    "    seed: int,\n",
    "    env: CascadeContextFreeBandit,\n",
    "    batch_size: int = 1\n",
    ") -> Random:\n",
    "\n",
    "    return Random(\n",
    "        n_actions=env.n_actions,\n",
    "        len_list=env.len_list,\n",
    "        random_state=seed,\n",
    "        batch_size=batch_size,\n",
    "    )    \n",
    "\n",
    "def get_ts_policy(\n",
    "    seed: int,\n",
    "    env: CascadeContextFreeBandit,\n",
    "    batch_size: int = 1,\n",
    "    alpha: int = 1,\n",
    "    beta: int = 1,\n",
    ") -> BernoulliTS:\n",
    "    return BernoulliTS(\n",
    "        n_actions=env.n_actions,\n",
    "        len_list=env.len_list,\n",
    "        random_state=seed,\n",
    "        batch_size=batch_size,\n",
    "        alpha=np.ones(env.n_actions) * alpha,\n",
    "        beta=np.ones(env.n_actions) * beta,\n",
    "    )\n",
    "    \n",
    "def play_episode(\n",
    "    env: CascadeContextFreeBandit,\n",
    "    policy: Union[Random, BernoulliTS],\n",
    "    seed: int,\n",
    ") -> list[ActionRewardLogging]:\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    action = policy.select_action()\n",
    "\n",
    "    reporting = []\n",
    "\n",
    "    while True:\n",
    "        _, reward, terminated, truncated, info = env.step(action=action)\n",
    "        reporting.append(dict(\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            prob_of_click=info[\"prob_of_click\"],        \n",
    "        ))\n",
    "\n",
    "        policy.cascade_params_update(\n",
    "            action=action,\n",
    "            reward_position=info[\"position_of_click\"]\n",
    "        )\n",
    "\n",
    "        if truncated:\n",
    "            break    \n",
    "        \n",
    "        action = policy.select_action()\n",
    "    return reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce time of an episode!\n",
    "n_episodes = 20\n",
    "episode_steps = 500_000\n",
    "env = CascadeContextFreeBandit(\n",
    "    weights=np.random.beta(a=1, b=99, size=N_ACTIONS),\n",
    "    max_steps=episode_steps,\n",
    "    len_list=LEN_LIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will simulate Beta(1, 1) and Beta(1, 99) but would like to consider another option as well.\n",
    "# Below we plot PDFs for different combinations of alpha and beta given a fixed sum of alpha + beta = 10 or 100.\n",
    "# NOTE: Beta (1, 1) assumes n_trials = successes + failures = 0 as by definition we have alpha = successes+1, beta = failures+1.\n",
    "# However, we could think of alpha=beta=1 as being the same as one success and one failure.\n",
    "# Also, the Beta distribution is defined for any alpha>0 and beta>0 - i.e. Beta(0.1, 0.1) would also create a valid distribution.\n",
    "# THE LATER can be exploited when warm starting a Beta Distribution for observed conversion rates\n",
    "\n",
    "pdfs = []\n",
    "alpha_max = 10\n",
    "for alpha in range(1, alpha_max):\n",
    "    for base in [alpha_max, 100]:\n",
    "        beta = base - alpha\n",
    "        mean = alpha / (alpha + beta)\n",
    "        title = f'Beta({alpha}, {beta})'\n",
    "        pdf_case_df = plot_beta_dist(alpha=alpha, beta=beta, size=100)\n",
    "        pdf_case_df = pdf_case_df.assign(\n",
    "            mean=mean, alpha=alpha, beta=beta, title=title, n_trials=alpha+beta,# really minus 2!\n",
    "        )\n",
    "        pdfs.append(pdf_case_df)\n",
    "pdfs_df = pd.concat(pdfs, axis=0).reset_index(drop=True)\n",
    "pdfs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(pdfs_df).mark_line().encode(\n",
    "    y=alt.Y('pdf'),\n",
    "    x=alt.X('x', title='θ'),\n",
    "    color=alt.Color(\n",
    "        'title',\n",
    "        scale=alt.Scale(\n",
    "            scheme=\"lightgreyred\", reverse=False\n",
    "        )\n",
    "    ),\n",
    "    tooltip=['title']\n",
    ").properties(width=700, height=200)\n",
    "chart = chart + chart.mark_bar(opacity=0.05)\n",
    "chart.facet(row='n_trials').resolve_scale(x='independent', y='independent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "POLICY_TYPE_TO_FUNCTION: dict[str, callable] = {\n",
    "    #\"random\": get_random_policy,\n",
    "    \"ts-priors-Beta(1, 1)\": partial(get_ts_policy, alpha=1, beta=1),\n",
    "    \"ts-priors-Beta(1, 9)\": partial(get_ts_policy, alpha=1, beta=9),    \n",
    "    \"ts-priors-Beta(1, 99)\": partial(get_ts_policy, alpha=1, beta=99),\n",
    "}\n",
    "\n",
    "episode_results = {\n",
    "    policy_type: []\n",
    "    for policy_type in POLICY_TYPE_TO_FUNCTION.keys()\n",
    "}\n",
    "\n",
    "\n",
    "for policy_type, policy_function in POLICY_TYPE_TO_FUNCTION.items():\n",
    "    for episode in tqdm.tqdm(range(n_episodes)):\n",
    "        policy = policy_function(env=env, seed=episode)\n",
    "        reporting = play_episode(\n",
    "            env=env,\n",
    "            policy=policy,\n",
    "            seed=episode,\n",
    "        )\n",
    "        episode_results[policy_type].append(reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting = []\n",
    "\n",
    "for policy_type, all_episode_reporting in episode_results.items():\n",
    "    for episode, episode_reporting in enumerate(all_episode_reporting):\n",
    "        print(f'processing: policy_type={policy_type}, episode={episode}', end='\\r')\n",
    "        \n",
    "        episode_reporting_df = harmonise_reporting(\n",
    "            reporting=episode_reporting,\n",
    "            env=env,\n",
    "            policy_batch_check=500\n",
    "        ).assign(episode=episode, policy_type=policy_type)\n",
    "        \n",
    "        reporting.append(episode_reporting_df)\n",
    "        \n",
    "reporting_df = pd.concat(reporting, axis=0).reset_index(drop=True)\n",
    "reporting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_df.groupby(['policy_type'], as_index=False)['episode'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_fields = [\n",
    "    'optimal_action_id',\n",
    "    'reward',\n",
    "    'cumulative_reward',\n",
    "    'avg_cumulative_reward',\n",
    "    'prob_of_click',\n",
    "    'optimal_prob_of_click',\n",
    "    'cumulative_optimal_action_taken',\n",
    "    'avg_cumulative_optimal_action_taken'\n",
    "]\n",
    "reporting_df2 = reporting_df.groupby(['policy_type', 'time_idx', ], as_index=False)[summary_fields].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 100\n",
    "mask_df = (reporting_df2['time_idx'] % plot_every) == 0\n",
    "mask_df.sum(), reporting_df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_df2[mask_df].query('time_idx>100')).mark_line().encode(\n",
    "    y=alt.Y('avg_cumulative_reward'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color=alt.Color('policy_type')\n",
    ").properties(width=400)\n",
    "\n",
    "\n",
    "chart_zoom = alt.Chart(reporting_df2[mask_df].query('time_idx>100000')).mark_line().encode(\n",
    "    y=alt.Y('avg_cumulative_reward', scale=alt.Scale(zero=False)),\n",
    "    x=alt.X('time_idx'),\n",
    "    color=alt.Color('policy_type')\n",
    ").properties(width=400)\n",
    "final_chart = chart | chart_zoom\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    final_chart, \n",
    "    f\"context_free_outputs/comparing_policies__avg_cumulative_reward.png\",\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chart = alt.Chart(reporting_df2[mask_df]).mark_line().encode(\n",
    "    y=alt.Y('prob_of_click', scale=alt.Scale(zero=False)),    \n",
    "    x=alt.X('time_idx'),\n",
    "    color=alt.Color('policy_type')\n",
    ").properties(width=400)\n",
    "\n",
    "\n",
    "chart_zoom = alt.Chart(reporting_df2[mask_df].query('time_idx>100000')).mark_line(opacity=0.8).encode(\n",
    "    y=alt.Y('prob_of_click', scale=alt.Scale(zero=False)),    \n",
    "    x=alt.X('time_idx'),\n",
    "    color=alt.Color('policy_type')\n",
    ").properties(width=400)\n",
    "\n",
    "final_chart = chart | chart_zoom\n",
    "\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    final_chart, \n",
    "    f\"context_free_outputs/comparing_policies__prob_of_click.png\",\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_1 = alt.Chart(reporting_df2[mask_df]).mark_line().encode(\n",
    "    y=alt.Y('cumulative_optimal_action_taken'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color=alt.Color('policy_type')\n",
    ").properties(width=400)\n",
    "\n",
    "chart_2 = alt.Chart(reporting_df2[mask_df]).mark_line().encode(\n",
    "    y=alt.Y('avg_cumulative_optimal_action_taken'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color=alt.Color('policy_type')\n",
    ").properties(width=400)\n",
    "\n",
    "chart = chart_1 | chart_2\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/comparing_policies__avg_cumulative_optimal_action_prob.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_action_prob_df = reporting_df.assign(n=1).groupby(['episode', 'policy_type', 'policy_batch_check'], as_index=False)[['n','optimal_action_id']].sum()\n",
    "optimal_action_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_action_prob_df = optimal_action_prob_df.assign(prob_optimal_action=lambda x: x['optimal_action_id']/x['n'])\n",
    "optimal_action_prob_df2 = optimal_action_prob_df.groupby(['policy_type', 'policy_batch_check', ], as_index=False)['prob_optimal_action'].mean()\n",
    "optimal_action_prob_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(optimal_action_prob_df2).mark_line().encode(\n",
    "    y=alt.Y('prob_optimal_action'),\n",
    "    x=alt.X('policy_batch_check'),\n",
    "    color=alt.Color('policy_type')\n",
    ").properties(width=400)\n",
    "\n",
    "chart_zoom = alt.Chart(optimal_action_prob_df2.query('policy_batch_check>200000')).mark_line().encode(\n",
    "    y=alt.Y('prob_optimal_action', scale=alt.Scale(zero=False)),    \n",
    "    x=alt.X('policy_batch_check'),\n",
    "    color=alt.Color('policy_type')\n",
    ").properties(width=400)\n",
    "\n",
    "final_chart = chart | chart_zoom\n",
    "\n",
    "vf.save_png(\n",
    "    final_chart, \n",
    "    f\"context_free_outputs/comparing_policies__batch_optimal_action_prob.png\",\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE FOR NOW:\n",
    "\n",
    "A way to get an alpha and beta given you have\n",
    "* an expected p/mean prob value\n",
    "* a belief of what the CDF should be given a p case which you wish the prob to be close to zero at that point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now cdf of beta distribution with above being alpha\n",
    "from scipy import stats\n",
    "from scipy.optimize import brentq\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observed_cdf_vs_expected(\n",
    "    beta: float,\n",
    "    mean: float,\n",
    "    cdf_value_evaluated_at_p: float = 0.8,\n",
    "    expected_cdf_value: float = 0.0001\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        beta (float): _description_\n",
    "        mean (float): _description_\n",
    "        cdf_value_evaluated_at_p (float, optional): _description_. Defaults to 0.8.\n",
    "        expected_cdf_value (float, optional): _description_. Defaults to 0.0001.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    alpha = get_alpha(mean=mean, beta=beta)\n",
    "    observed_cdf = stats.beta.cdf(cdf_value_evaluated_at_p, alpha, beta)\n",
    "    return  observed_cdf - expected_cdf_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brute_force_approach = []\n",
    "for beta in range(1, 100):\n",
    "    error = observed_cdf_vs_expected(\n",
    "        beta=beta, mean=0.95, cdf_value_evaluated_at_p=0.8, expected_cdf_value=0.0001\n",
    "    )\n",
    "    brute_force_approach.append(dict(beta=beta, error=error))\n",
    "\n",
    "pd.DataFrame(brute_force_approach).sort_values('error').query('error>0').reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brentq(\n",
    "    partial(\n",
    "        observed_cdf_vs_expected,\n",
    "        mean=0.95,\n",
    "        cdf_value_evaluated_at_p=0.8,\n",
    "        expected_cdf_value=0.0001,\n",
    "    ),\n",
    "    1,\n",
    "    100,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
