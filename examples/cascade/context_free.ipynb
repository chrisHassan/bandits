{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from bandits.environment.cascade.context_free import CascadeContextFreeBandit\n",
    "from bandits.policy.context_free import BernoulliTS, Random\n",
    "from bandits.plotting import plot_beta_dist\n",
    "import vegafusion as vf\n",
    "from typing import TypedDict\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.enable('json') # saves the data locally in .json file so notebook doesn't get large\n",
    "# or can allow altair to keep the data in the notebook using alt.data_transformers.disable_max_rows() but it will create large notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf_vs_actual(\n",
    "    pdf_df: pd.DataFrame,\n",
    "    actuals_df: pd.DataFrame,\n",
    "    width: int =500,\n",
    "    height: int =250\n",
    ") -> alt.Chart:\n",
    "        \n",
    "    pdf_charts = alt.Chart(pdf_df).mark_line().encode(\n",
    "        y=alt.Y('pdf'),\n",
    "        x=alt.X('x', title='θ'),\n",
    "        color=alt.Color('action:N', legend=None),\n",
    "        tooltip = [\n",
    "            'action',\n",
    "            alt.Tooltip('pdf', format='0.4', title='θ'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    actual_charts = alt.Chart(actuals_df).mark_rule().encode(\n",
    "        x=alt.X('w', title='θ'),\n",
    "        color=alt.Color('action:N', legend=None),\n",
    "        tooltip = [\n",
    "            'action',\n",
    "            alt.Tooltip('w', format='0.4', title='θ'),\n",
    "        ]        \n",
    "    )\n",
    "\n",
    "    final_chart = (pdf_charts + actual_charts).properties(\n",
    "        width=width, height=height\n",
    "    )\n",
    "\n",
    "    return final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(\n",
    "    policy: BernoulliTS,\n",
    "    env: CascadeContextFreeBandit\n",
    ") -> alt.Chart:\n",
    "\n",
    "    pred = (policy.reward_counts / policy.action_counts)\n",
    "    act = env.weights\n",
    "\n",
    "    chart_df = pd.DataFrame(dict(pred=pred, act=act)).assign(\n",
    "        diff=lambda x: (x.act - x.pred).abs()\n",
    "    ).assign(\n",
    "        arm=lambda x: x.index,\n",
    "        optimal_arm=lambda x: x.arm.isin(env.optimal_action),\n",
    "        act_sort=lambda x: x.act,\n",
    "    ).sort_values(\n",
    "        ['diff'], ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    chart_ff_df = chart_df.melt(id_vars=['arm', 'optimal_arm', 'diff', 'act_sort'])\n",
    "\n",
    "    return alt.Chart(chart_ff_df).mark_point().encode(\n",
    "        y=alt.Y('arm:O', sort=alt.SortField(\"act_sort\", \"descending\")),\n",
    "        x=alt.X('value'),\n",
    "        color=alt.Color('variable'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observed_optimal_action_prob(\n",
    "    reporting_df: pd.DataFrame,\n",
    "    height: int = 275,\n",
    "    width: int = 675,    \n",
    ") -> tuple[pd.DataFrame, alt.Chart]:\n",
    "    policy_prob_df = reporting_df.assign(\n",
    "        n_trials=1\n",
    "    ).groupby(\n",
    "        ['policy_batch_check'], as_index=False\n",
    "    )[['n_trials','optimal_action_id']].sum().assign(\n",
    "        prob_of_optimal_action=lambda x: x['optimal_action_id'] /  x['n_trials']\n",
    "    )\n",
    "\n",
    "    chart = alt.Chart(policy_prob_df).mark_line().encode(\n",
    "        x='policy_batch_check', y='prob_of_optimal_action'\n",
    "    ).properties(\n",
    "        width=width, \n",
    "        height=height,\n",
    "    )\n",
    "    return policy_prob_df, chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf_with_actuals(\n",
    "    policy: BernoulliTS,\n",
    "    env: CascadeContextFreeBandit,\n",
    "    width: int = 500,\n",
    "    height: int = 250,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, alt.Chart]:\n",
    "    actuals_df = pd.DataFrame(dict(w=env.weights)).assign(action=lambda x: x.index)\n",
    "\n",
    "    all_pdf = []\n",
    "    for idx in range(policy.n_actions):\n",
    "        pdf = plot_beta_dist(\n",
    "            alpha=policy.alpha[idx] + policy.reward_counts[idx],\n",
    "            beta=policy.beta[idx] + (policy.action_counts[idx] - policy.reward_counts[idx])\n",
    "        ).assign(\n",
    "            action=idx,\n",
    "        )\n",
    "        all_pdf.append(pdf)\n",
    "\n",
    "    all_pdf_df = pd.concat(all_pdf, axis=0).reset_index(drop=True)    \n",
    "\n",
    "    pdf_charts = alt.Chart(all_pdf_df).mark_line().encode(\n",
    "        y=alt.Y('pdf'),\n",
    "        x=alt.X('x', title='θ'),\n",
    "        color=alt.Color('action:N', legend=None)\n",
    "    )\n",
    "\n",
    "    actual_charts = alt.Chart(actuals_df).mark_rule().encode(\n",
    "        x=alt.X('w', title='θ'),\n",
    "        color=alt.Color('action:N', legend=None)\n",
    "    )\n",
    "\n",
    "    final_chart = (pdf_charts + actual_charts).properties(\n",
    "        width=width, height=height\n",
    "    )\n",
    "\n",
    "    return all_pdf_df, actuals_df, final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionRewardLogging(TypedDict):\n",
    "    action: list[int]\n",
    "    reward: float\n",
    "    prob_of_click: float\n",
    "\n",
    "def harmonise_reporting(\n",
    "    reporting: list[ActionRewardLogging],\n",
    "    env: CascadeContextFreeBandit,\n",
    "    policy_batch_check: int = 500,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    reporting_df = pd.DataFrame(reporting).assign(\n",
    "        optimal_prob_of_click=env.optimal_reward,\n",
    "        time_idx=lambda x: x.index\n",
    "    )\n",
    "    reporting_df['avg_reward'] = reporting_df['reward'].cumsum() / reporting_df['time_idx']\n",
    "    reporting_df['avg_reward'] = reporting_df['avg_reward'].fillna(0)\n",
    "    reporting_df['action_as_str'] = reporting_df['action'].apply(lambda x: '|'.join([str(y) for y in x]))\n",
    "    reporting_df['optimal_action_id'] = reporting_df['action'].apply(lambda x: all(x == env.optimal_action))\n",
    "    reporting_df['policy_batch_check'] = (reporting_df['time_idx']  - reporting_df['time_idx'] % policy_batch_check)\n",
    "    return reporting_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 50\n",
    "LEN_LIST = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "env = CascadeContextFreeBandit(\n",
    "    weights=np.random.beta(a=1, b=99, size=N_ACTIONS),\n",
    "    max_steps=1_000_000,\n",
    "    len_list=LEN_LIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_of_policies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_df = pd.DataFrame(dict(w=env.weights)).assign(action=lambda x: x.index)\n",
    "actuals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = N_ACTIONS * 13\n",
    "actual_charts = alt.Chart(actuals_df).mark_rule().encode(\n",
    "    x=alt.X(\n",
    "        'action:N',\n",
    "        sort=alt.SortField(\"w\", order='descending'),\n",
    "        axis=alt.Axis(orient='bottom', labelAngle=0)\n",
    "    ),\n",
    "    color=alt.Color('action:N', legend=None),\n",
    "    y=alt.Y('w', title='θ'),\n",
    "    text=alt.Text('w', format='0.3'),\n",
    "    tooltip=[\n",
    "        'action',\n",
    "        alt.Tooltip('w', format='0.3', title='θ')\n",
    "    ]\n",
    ")\n",
    "\n",
    "final_chart = (\n",
    "    actual_charts +\n",
    "    actual_charts.mark_point(filled=True, size=50) + \n",
    "    actual_charts.mark_text(align='left', angle=45*7, dx=5)\n",
    ").properties(\n",
    "    width=width, height=225\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Random(\n",
    "    n_actions=env.n_actions,\n",
    "    len_list=env.len_list,\n",
    "    random_state=1234,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=1234)\n",
    "action = policy.select_action()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting = []\n",
    "\n",
    "while True:\n",
    "    _, reward, terminated, truncated, info = env.step(action=action)\n",
    "    reporting.append(dict(\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        prob_of_click=info[\"prob_of_click\"],        \n",
    "    ))\n",
    "\n",
    "    policy.cascade_params_update(\n",
    "        action=action,\n",
    "        reward_position=info[\"position_of_click\"]\n",
    "    )\n",
    "\n",
    "    if truncated:\n",
    "        break    \n",
    "\n",
    "    action = policy.select_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_df = harmonise_reporting(\n",
    "    reporting=reporting,\n",
    "    env=env,\n",
    "    policy_batch_check=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'random'\n",
    "outputs_of_policies[policy_name] = reporting_df.assign(policy=policy_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_ff_df = reporting_df.melt(\n",
    "    id_vars=['time_idx', 'action'],\n",
    "    value_vars=['prob_of_click', 'optimal_prob_of_click']\n",
    ")\n",
    "\n",
    "reporting_ff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 1_000\n",
    "mask_df = (reporting_df['time_idx'] % plot_every) == 0\n",
    "mask_ff_df = (reporting_ff_df['time_idx'] % plot_every) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_ff_df[mask_ff_df]).mark_line().encode(\n",
    "    y=alt.Y('value'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color='variable'\n",
    ").properties(width=600)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__prob_of_click.png\",\n",
    ")\n",
    "\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_df[mask_df]).mark_line().encode(\n",
    "    x='time_idx',\n",
    "    y='avg_reward',\n",
    ").properties(\n",
    "    width=700, \n",
    ")\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__avg_reward.png\",\n",
    ")\n",
    "\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = plot_actual_vs_predicted(policy=policy, env=env)\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_probs_actual_vs_predicted.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_prob_df, chart = plot_observed_optimal_action_prob(\n",
    "    reporting_df=reporting_df,\n",
    "    height=275,\n",
    "    width=675\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_prob_df['optimal_action_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uninformed Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BernoulliTS(\n",
    "    n_actions=env.n_actions,\n",
    "    len_list=env.len_list,\n",
    "    random_state=1234,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_df = plot_beta_dist(alpha=1, beta=1)\n",
    "actuals_df = pd.DataFrame(dict(w=env.weights)).assign(action=lambda x: x.index)\n",
    "\n",
    "actuals_chart = alt.Chart(actuals_df).mark_rule(opacity=0.5).encode(\n",
    "    x=alt.X('w', title='θ'),\n",
    ")\n",
    "priors_chart = alt.Chart(prior_df).mark_area(opacity=0.5).encode(\n",
    "    x=alt.X('x', title='θ'),\n",
    "    y=alt.Y('pdf')\n",
    ")\n",
    "\n",
    "final_chart = (\n",
    "    priors_chart + actuals_chart\n",
    ").properties(\n",
    "    width=500, height=200\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=34325)\n",
    "action = policy.select_action()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting = []\n",
    "\n",
    "while True:\n",
    "    _, reward, terminated, truncated, info = env.step(action=action)\n",
    "    reporting.append(dict(\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        prob_of_click=info[\"prob_of_click\"],        \n",
    "    ))\n",
    "\n",
    "    policy.cascade_params_update(\n",
    "        action=action,\n",
    "        reward_position=info[\"position_of_click\"]\n",
    "    )\n",
    "\n",
    "    if truncated:\n",
    "        break    \n",
    "    \n",
    "    action = policy.select_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_df = harmonise_reporting(reporting=reporting, env=env, policy_batch_check=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'ts-priors-Beta(1, 1)'\n",
    "outputs_of_policies[policy_name] = reporting_df.assign(policy=policy_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_ff_df = reporting_df.melt(\n",
    "    id_vars=['time_idx', 'action'],\n",
    "    value_vars=['prob_of_click', 'optimal_prob_of_click']\n",
    ")\n",
    "\n",
    "reporting_ff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 100\n",
    "mask_df = (reporting_df['time_idx'] % plot_every) == 0\n",
    "mask_ff_df = (reporting_ff_df['time_idx'] % plot_every) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_ff_df[mask_ff_df]).mark_line().encode(\n",
    "    y=alt.Y('value'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color='variable'\n",
    ").properties(width=600)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__prob_of_click.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_df[mask_df]).mark_line().encode(\n",
    "    x='time_idx',\n",
    "    y='avg_reward',\n",
    ").properties(\n",
    "    width=700, \n",
    ")\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__avg_reward.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = plot_actual_vs_predicted(policy=policy, env=env)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_probs_actual_vs_predicted.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_prob_df, chart = plot_observed_optimal_action_prob(\n",
    "    reporting_df=reporting_df,\n",
    "    height=275,\n",
    "    width=675\n",
    ")\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__observed_optimal_action_distribution.png\",\n",
    ")\n",
    "\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf_df, actuals_df, chart = plot_pdf_with_actuals(env=env, policy=policy)\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_beta_distributions.png\",\n",
    ")\n",
    "\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pessimistic priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BernoulliTS(\n",
    "    n_actions=env.n_actions,\n",
    "    len_list=env.len_list,\n",
    "    random_state=1234,\n",
    "    batch_size=1,\n",
    "    alpha=np.ones(env.n_actions) * 1,\n",
    "    beta=np.ones(env.n_actions) * 99,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_df = plot_beta_dist(alpha=1, beta=99)\n",
    "actuals_df = pd.DataFrame(dict(w=env.weights)).assign(action=lambda x: x.index)\n",
    "\n",
    "actuals_chart = alt.Chart(actuals_df).mark_rule(opacity=0.5).encode(\n",
    "    x=alt.X('w', title='θ'),\n",
    ")\n",
    "priors_chart = alt.Chart(prior_df).mark_area(opacity=0.5).encode(\n",
    "    x=alt.X('x', title='θ'),\n",
    "    y=alt.Y('pdf')\n",
    ")\n",
    "\n",
    "final_chart = (\n",
    "    priors_chart + actuals_chart\n",
    ").properties(\n",
    "    width=500, height=200\n",
    ")\n",
    "\n",
    "final_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=1234)\n",
    "action = policy.select_action()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting = []\n",
    "\n",
    "while True:\n",
    "    _, reward, terminated, truncated, info = env.step(action=action)\n",
    "    reporting.append(dict(\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        prob_of_click=info[\"prob_of_click\"],        \n",
    "    ))\n",
    "\n",
    "    policy.cascade_params_update(\n",
    "        action=action,\n",
    "        reward_position=info[\"position_of_click\"]\n",
    "    )\n",
    "\n",
    "    if truncated:\n",
    "        break    \n",
    "    \n",
    "    action = policy.select_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = 'ts-priors-Beta(1, 99)'\n",
    "outputs_of_policies[policy_name] = reporting_df.assign(policy=policy_name)\n",
    "\n",
    "reporting_ff_df = reporting_df.melt(\n",
    "    id_vars=['time_idx', 'action'],\n",
    "    value_vars=['prob_of_click', 'optimal_prob_of_click']\n",
    ")\n",
    "\n",
    "reporting_ff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 100\n",
    "mask_df = (reporting_df['time_idx'] % plot_every) == 0\n",
    "mask_ff_df = (reporting_ff_df['time_idx'] % plot_every) == 0\n",
    "\n",
    "\n",
    "chart = alt.Chart(reporting_ff_df[mask_ff_df]).mark_line().encode(\n",
    "    y=alt.Y('value'),\n",
    "    x=alt.X('time_idx'),\n",
    "    color='variable'\n",
    ").properties(width=600)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__prob_of_click.png\",\n",
    ")\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(reporting_df[mask_df]).mark_line().encode(\n",
    "    x='time_idx',\n",
    "    y='avg_reward',\n",
    ").properties(\n",
    "    width=700, \n",
    ")\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__avg_reward.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = plot_actual_vs_predicted(policy=policy, env=env)\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_probs_actual_vs_predicted.png\",\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_prob_df, chart = plot_observed_optimal_action_prob(\n",
    "    reporting_df=reporting_df,\n",
    "    height=275,\n",
    "    width=675\n",
    ")\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__observed_optimal_action_distribution.png\",\n",
    ")\n",
    "\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pdf_df, actuals_df, chart = plot_pdf_with_actuals(env=env, policy=policy)\n",
    "\n",
    "\n",
    "vf.save_png(\n",
    "    chart, \n",
    "    f\"context_free_outputs/{policy_name}__action_beta_distributions.png\",\n",
    ")\n",
    "\n",
    "\n",
    "chart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
